{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drshikaasher/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import supabase\n",
    "import os \n",
    "from datasets import Dataset \n",
    "from ragas.metrics import faithfulness, context_precision, answer_relevancy\n",
    "from ragas import evaluate\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Supabase Client\n",
    "# supabase_client = supabase.create_client(supabase_url=\"redacted\", supabase_key=\"redacted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# page_size = 100\n",
    "# last_id = 0\n",
    "# all_data = []\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         # Fetch data using cursor-based pagination\n",
    "#         response = supabase_client.table(\"llm-convo-monitor\") \\\n",
    "#             .select(\"id\", \"convo\") \\\n",
    "#             .eq(\"course_name\", \"cropwizard-1.5\") \\\n",
    "#             .order(\"id\") \\\n",
    "#             .gt(\"id\", last_id) \\\n",
    "#             .limit(page_size) \\\n",
    "#             .execute()\n",
    "        \n",
    "#         # Check if the response is valid\n",
    "#         if not response.data:\n",
    "#             print(\"No data returned. Breaking the loop.\")\n",
    "#             break\n",
    "        \n",
    "#         # Append the fetched data to the result\n",
    "#         all_data.extend(response.data)\n",
    "        \n",
    "#         # Update the last_id for the next iteration\n",
    "#         last_id = response.data[-1]['id']\n",
    "        \n",
    "#         print(f\"Fetched {len(response.data)} records. Last ID: {last_id}\")\n",
    "        \n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON Decode Error: {str(e)}\")\n",
    "#         print(\"Response content:\", response.content)\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#         break\n",
    "\n",
    "# # Print the total number of records fetched\n",
    "# print(f\"Total records fetched: {len(all_data)}\")\n",
    "\n",
    "# # Use all_data for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 967 records from data.json\n"
     ]
    }
   ],
   "source": [
    "# Question -> conversation.messages[0].content[0].text\n",
    "# Answer -> conversation.messages[1].content\n",
    "# contexts -> conversation.messages[0].contexts # this is an array of dicts\n",
    "\n",
    "# metrics=[context_precision, faithfulness, answer_relevancy]\n",
    "\n",
    "# with open('data.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "# Load data from data.json\n",
    "\n",
    "with open('data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)  # Load only the first 10 objects\n",
    "\n",
    "print(f\"Loaded {len(all_data)} records from data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|▍         | 71/1503 [04:57<1:51:20,  4.66s/it]Exception raised in Job[1289]: TimeoutError()\n",
      "Evaluating:  10%|▉         | 149/1503 [09:25<1:26:04,  3.81s/it]Exception raised in Job[1499]: TimeoutError()\n",
      "Evaluating:  10%|█         | 152/1503 [09:33<1:08:54,  3.06s/it]Exception raised in Job[1502]: TimeoutError()\n",
      "Evaluating:  18%|█▊        | 267/1503 [16:53<43:21,  2.11s/it]  Exception raised in Job[1304]: TimeoutError()\n",
      "Evaluating:  18%|█▊        | 278/1503 [17:28<39:14,  1.92s/it]  Exception raised in Job[1310]: TimeoutError()\n",
      "Evaluating:  19%|█▉        | 285/1503 [17:58<1:15:31,  3.72s/it]Exception raised in Job[1313]: TimeoutError()\n",
      "Evaluating:  20%|██        | 303/1503 [19:24<2:00:05,  6.00s/it]Exception raised in Job[1322]: TimeoutError()\n",
      "Evaluating:  21%|██        | 316/1503 [20:12<59:21,  3.00s/it]  Exception raised in Job[1328]: TimeoutError()\n",
      "Evaluating:  21%|██        | 318/1503 [20:17<57:42,  2.92s/it]Exception raised in Job[1331]: TimeoutError()\n",
      "Evaluating:  24%|██▍       | 366/1503 [22:50<37:26,  1.98s/it]  No statements were generated from the answer.\n",
      "Evaluating:  37%|███▋      | 551/1503 [33:48<38:42,  2.44s/it]  No statements were generated from the answer.\n",
      "Evaluating:  37%|███▋      | 557/1503 [34:24<1:44:09,  6.61s/it]Exception raised in Job[1358]: TimeoutError()\n",
      "Evaluating:  38%|███▊      | 569/1503 [34:43<33:33,  2.16s/it]  Exception raised in Job[1359]: TimeoutError()\n",
      "Evaluating:  38%|███▊      | 572/1503 [34:54<45:11,  2.91s/it]Exception raised in Job[1361]: TimeoutError()\n",
      "Evaluating:  41%|████      | 613/1503 [37:01<39:47,  2.68s/it]  No statements were generated from the answer.\n",
      "Evaluating:  48%|████▊     | 720/1503 [43:25<43:39,  3.35s/it]  Exception raised in Job[1391]: TimeoutError()\n",
      "Evaluating:  49%|████▉     | 733/1503 [44:04<21:19,  1.66s/it]  No statements were generated from the answer.\n",
      "Evaluating:  49%|████▉     | 738/1503 [44:21<44:35,  3.50s/it]No statements were generated from the answer.\n",
      "Evaluating:  56%|█████▌    | 840/1503 [49:26<55:07,  4.99s/it]  No statements were generated from the answer.\n",
      "Evaluating:  79%|███████▉  | 1193/1503 [1:09:30<20:31,  3.97s/it]Exception raised in Job[1455]: TimeoutError()\n",
      "Evaluating:  92%|█████████▏| 1388/1503 [1:19:50<09:16,  4.84s/it]Exception raised in Job[1457]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 1503/1503 [1:26:44<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated score\n",
      "final scores {'faithfulness': 0.7710, 'answer_relevancy': 0.7267, 'context_precision': 0.3007}\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"redacted\"\n",
    "\n",
    "data_samples = {\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'contexts': [],\n",
    "    'reference': []\n",
    "}\n",
    "\n",
    "for index, item in enumerate(all_data):\n",
    "    try:\n",
    "        if len(item['convo']['messages']) < 2:\n",
    "            continue\n",
    "        \n",
    "        question_content = item['convo']['messages'][0]['content']\n",
    "        if not isinstance(question_content, list) or len(question_content) == 0 or 'text' not in question_content[0]:\n",
    "            continue\n",
    "        question = question_content[0]['text']\n",
    "        if not isinstance(question, str):\n",
    "            continue\n",
    "        \n",
    "        answer_content = item['convo']['messages'][1]['content']\n",
    "        if isinstance(answer_content, list):\n",
    "            answer = ' '.join([content.get('text', '') for content in answer_content if content.get('type') == 'text'])\n",
    "        elif isinstance(answer_content, str):\n",
    "            answer = answer_content\n",
    "        else:\n",
    "            continue\n",
    "        contexts = item['convo']['messages'][0].get('contexts', [])\n",
    "        \n",
    "        data_samples['question'].append(question)\n",
    "        data_samples['answer'].append(answer)\n",
    "        flattened_contexts = [context['text'] for context in contexts]\n",
    "        # print(flattened_contexts)\n",
    "        data_samples['contexts'].append(flattened_contexts)\n",
    "        data_samples['reference'].append('')  # Placeholder for reference\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# print(f\"Processed items: question: {len(data_samples['question'])}, answer: {len(data_samples['answer'])}, contexts: {len(data_samples['contexts'])}, reference: {len(data_samples['reference'])}\")\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "min_length = min(len(data_samples[key]) for key in data_samples)\n",
    "for key in data_samples:\n",
    "    data_samples[key] = data_samples[key][:min_length]\n",
    "\n",
    "# print(f\"Final lengths: question: {len(data_samples['question'])}, answer: {len(data_samples['answer'])}, contexts: {len(data_samples['contexts'])}, reference: {len(data_samples['reference'])}\")\n",
    "\n",
    "try:\n",
    "    dataset = Dataset.from_dict(data_samples)\n",
    "    print(\"made dataset\")\n",
    "    score_r = evaluate(dataset,metrics=[faithfulness, answer_relevancy, context_precision])\n",
    "    print(\"calculated score\")\n",
    "    print(\"final scores\", score_r)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the dataset: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    # for key, value in data_samples.items():\n",
    "    #     print(f\"{key}: {type(value)}, length: {len(value)}\")\n",
    "    #     if value:\n",
    "    #         print(f\"First {key} item type: {type(value[0])}\")\n",
    "    #         print(f\"First {key} item: {value[0][:100]}...\")  # Print first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
